{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-09dd2fad33c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import json\n",
    "import torchtext.vocab\n",
    "\n",
    "import data.gen_one_hot\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_path = \"data//9_objs//train.json\"\n",
    "dev_data_path = \"data//9_objs//dev.json\"\n",
    "test_data = \"data//9_objs//test.json\"\n",
    "all_data = \"data//9_objs//final_data.json\"\n",
    "\n",
    "# We will generate our own one-hot embeddings and throw out the ones that are already in the .json files, if there are any\n",
    "g = data.gen_one_hot.Gen_data(all_data)\n",
    "print(\"output form G\",g)\n",
    "\n",
    "# all the one-hot embedding etc. has to be handled here, rather in data generation,\n",
    "# because we have to know which word indice go to which real words in order to do the embedding.\n",
    "\n",
    "#--------------- Model --------------\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_lens = [30,40,50], nonlins = [nn.Tanh(), nn.ReLU()], drop_freqs= [.5,.6]):\n",
    "        super(Neural_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.act_ftns = nonlins\n",
    "        self.dropouts = []\n",
    "\n",
    "        curr_layer_len = input_size\n",
    "        for l in layer_lens:\n",
    "            self.layers.append(nn.Linear(curr_layer_len, l, bias = True))\n",
    "            curr_layer_len = l\n",
    "        self.layers.append( nn.Linear(curr_layer_len,self.output_size))\n",
    "\n",
    "        for f in drop_freqs:\n",
    "             self.dropouts.append(nn.Dropout(f))\n",
    "\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        import itertools\n",
    "        components = list(itertools.zip_longest(self.layers,self.act_ftns, self.dropouts))\n",
    "        for layer, act_ftn, dropout in components:\n",
    "            #init components and forward through\n",
    "            if layer != None:\n",
    "                x = layer(x)\n",
    "            if act_ftn != None:\n",
    "                x = act_ftn(x)\n",
    "            if dropout != None:\n",
    "                x = dropout(x)\n",
    "        x = nn.LogSoftmax()(x)\n",
    "        return x\n",
    "\n",
    "#------------------ Loss & Optimizer --------------------\n",
    "def cross_entropy_loss(logp_hats, ps):\n",
    "    return torch.mean(torch.sum(- ps * logp_hats, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------- Metrics ---------------------\n",
    "def cross_entropy(log_pred, targets):\n",
    "    log_pred = Variable(torch.Tensor(log_pred))\n",
    "    targets = Variable(torch.Tensor(targets))\n",
    "\n",
    "    result = sum(torch.sum(-targets * log_pred, 1).data.numpy()) #I should as Professor Singh for help here as torch.sum is returning an array over the 6 instances. Does summing those before returning them make sense? And does it make sense to take the average once all those instances have been collected (as shown in line 86). Right now it doesn't make sense because cross entropy is really high compared to mse\n",
    "    return result\n",
    "\n",
    "def accuracy(pred, targets):\n",
    "    return np.argmax(pred) == np.argmax(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = eval(open('fig_5_labels.json','r').read())\n",
    "contexts = list(data.keys())\n",
    "contexts.sort()\n",
    "utters = [1,2,3,4,5]\n",
    "states = [1,2,3,4,5]\n",
    "\n",
    "def build_one_hot_utter(u):\n",
    "    one_hot_utter = np.zeros(len(utters))\n",
    "    one_hot_utter[utters.index(u)] = 1\n",
    "    return one_hot_utter\n",
    "def build_one_hot_state(s):\n",
    "    one_hot_state = np.zeros(len(states))\n",
    "    one_hot_state[states.index(s)] = 1\n",
    "    return one_hot_state\n",
    "def build_one_hot_context(c):\n",
    "    one_hot_context = np.zeros(len(contexts))\n",
    "    one_hot_context[contexts.index(c)] = 1\n",
    "    return one_hot_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for context, vals in data.items():\n",
    "    one_hot_c = build_one_hot_context(context)\n",
    "    for utter, state in vals.items():\n",
    "        one_hot_u = build_one_hot_utter(utter)\n",
    "        x.append(np.concatenate((one_hot_c,one_hot_u),axis=None))\n",
    "        y.append(state)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 14)\n",
      "(45, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
