{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import json\n",
    "import torchtext.vocab\n",
    "\n",
    "\n",
    "SAVE_PATH = 'models/model_'\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# all the one-hot embedding etc. has to be handled here, rather in data generation,\n",
    "# because we have to know which word indice go to which real words in order to do the embedding.\n",
    "\n",
    "#--------------- Model --------------\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_lens = [30,40,50], nonlins = [nn.Tanh(), nn.ReLU()], drop_freqs= [.5,.6]):\n",
    "        super(Neural_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.act_ftns = nonlins\n",
    "        self.dropouts = []\n",
    "\n",
    "        curr_layer_len = input_size\n",
    "        for l in layer_lens:\n",
    "            self.layers.append(nn.Linear(curr_layer_len, l, bias = True))\n",
    "            curr_layer_len = l\n",
    "        self.layers.append( nn.Linear(curr_layer_len,self.output_size))\n",
    "\n",
    "        for f in drop_freqs:\n",
    "             self.dropouts.append(nn.Dropout(f))\n",
    "\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        import itertools\n",
    "        components = list(itertools.zip_longest(self.layers,self.act_ftns, self.dropouts))\n",
    "        for layer, act_ftn, dropout in components:\n",
    "            #init components and forward through\n",
    "            if layer != None:\n",
    "                x = layer(x)\n",
    "            if act_ftn != None:\n",
    "                x = act_ftn(x)\n",
    "            if dropout != None:\n",
    "                x = dropout(x)\n",
    "        x = nn.LogSoftmax()(x)\n",
    "        return x\n",
    "\n",
    "#------------------ Loss & Optimizer --------------------\n",
    "def cross_entropy_loss(pred, soft_targets):\n",
    "    logsoftmax = nn.LogSoftmax()\n",
    "    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Metrics ---------------------\n",
    "def cross_entropy(y_true,y_hat,  eps=1e-15):\n",
    "    return -(y_true * np.log(y_hat)).sum(axis=1).mean()\n",
    "\n",
    "\n",
    "def accuracy(pred, targets):\n",
    "    return sum(np.argmax(pred,axis=1) == np.argmax(targets,axis=1))/len(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = eval(open('fig_5_labels.json','r').read())\n",
    "contexts = eval(open('irony/prior_states.json').read())\n",
    "utters = [1,2,3,4,5]\n",
    "states = [1,2,3,4,5]\n",
    "\n",
    "def build_one_hot_utter(u):\n",
    "    one_hot_utter = np.zeros(len(utters))\n",
    "    one_hot_utter[utters.index(u)] = 1\n",
    "    return one_hot_utter\n",
    "def build_one_hot_state(s):\n",
    "    one_hot_state = np.zeros(len(states))\n",
    "    one_hot_state[states.index(s)] = 1\n",
    "    return one_hot_state\n",
    "def build_context(c):\n",
    "    priors_dict = contexts[c]\n",
    "    state_priors = [priors_dict[s] for s in states]\n",
    "    return state_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for c, vals in data.items():\n",
    "    context_prior = build_context(c)\n",
    "    for utter, state in vals.items():\n",
    "        one_hot_u = build_one_hot_utter(utter)\n",
    "        x.append(np.concatenate((context_prior,one_hot_u),axis=None))\n",
    "        y.append(state)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_data, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        x = Variable(torch.Tensor(training_data[0]))\n",
    "        y = Variable(torch.Tensor(training_data[1]))\n",
    "\n",
    "        log_y_pred = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(log_y_pred, y)\n",
    "        print(epoch, loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_sheet(model, train_metrics = (\"N/A\",\"N/A\",\"N/A\"), dev_metrics= (\"N/A\",\"N/A\",\"N/A\"), test_metrics= (\"N/A\",\"N/A\",\"N/A\")):\n",
    "    import gspread\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    wks = gc.open('RSA-NN').sheet1\n",
    "\n",
    "    row = str(len(wks.get_all_records()) +2)\n",
    "    \n",
    "    torch.save(model.state_dict(),SAVE_PATH+row+'.pt')\n",
    "\n",
    "    wks.append_row([\n",
    "                train_metrics[0], train_metrics[1], train_metrics[2],\n",
    "                dev_metrics[0], dev_metrics[1], dev_metrics[2],\n",
    "                test_metrics[0], test_metrics[1], test_metrics[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    x = Variable(torch.Tensor(data[0]))\n",
    "    y = data[1]\n",
    "    \n",
    "    y_hat = model(x).data.numpy()\n",
    "    \n",
    "    ce = cross_entropy(y_hat,y)\n",
    "    acc = accuracy(y_hat,y)\n",
    "    mse = metrics.mean_squared_error(y_hat,y)\n",
    "    print(\"ce: \", ce)\n",
    "    print(\"acc: \", acc)\n",
    "    print(\"mse: \", mse)\n",
    "    append_to_sheet(model,(acc,mse,ce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  -49.379173443465604\n",
      "acc:  0.8444444444444444\n",
      "mse:  11.354224718051302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  -42.88926511532396\n",
      "acc:  0.7777777777777778\n",
      "mse:  8.97692709677953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  -48.38013969659788\n",
      "acc:  0.8666666666666667\n",
      "mse:  10.684594520143417\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    loss_fn = cross_entropy_loss\n",
    "    for learning_rate in [1e-3,1e-4,1e-2]:\n",
    "        for nonlin in [[nn.Tanh(),nn.ReLU()]]:\n",
    "            for num_units in [[70,80,90]]:\n",
    "                model = Neural_Net(x.shape[1],y.shape[1],layer_lens=num_units,\n",
    "                                  nonlins=nonlin).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=.0001)\n",
    "                train(model,optimizer, loss_fn, (x,y), num_epochs=NUM_EPOCHS)\n",
    "                evaluate(model,(x,y))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  -43.95086988374255\n",
      "acc:  0.8\n",
      "mse:  9.053936590332864\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    loss_fn = cross_entropy_loss\n",
    "    for learning_rate in [1e-2]:\n",
    "        for nonlin in [[nn.ReLU()]]:\n",
    "            for num_units in [[70,50,90]]:\n",
    "                model = Neural_Net(x.shape[1],y.shape[1],layer_lens=num_units,\n",
    "                                  nonlins=nonlin).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=.0001)\n",
    "                train(model,optimizer, loss_fn, (x,y), num_epochs=NUM_EPOCHS)\n",
    "                evaluate(model,(x,y))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross entropy lower bound\n",
    "z = Variable(torch.Tensor(y))\n",
    "cross_entropy_loss(z,z)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
