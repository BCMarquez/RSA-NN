{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import json\n",
    "import torchtext.vocab\n",
    "\n",
    "\n",
    "SAVE_PATH = 'models/model_'\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# all the one-hot embedding etc. has to be handled here, rather in data generation,\n",
    "# because we have to know which word indice go to which real words in order to do the embedding.\n",
    "\n",
    "#--------------- Model --------------\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_lens = [30,40,50], nonlins = [nn.Tanh(), nn.ReLU()], drop_freqs= []):\n",
    "        super(Neural_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.act_ftns = nonlins\n",
    "        self.dropouts = []\n",
    "\n",
    "        curr_layer_len = input_size\n",
    "        for l in layer_lens:\n",
    "            self.layers.append(nn.Linear(curr_layer_len, l, bias = True))\n",
    "            curr_layer_len = l\n",
    "        self.layers.append( nn.Linear(curr_layer_len,self.output_size))\n",
    "\n",
    "        for f in drop_freqs:\n",
    "             self.dropouts.append(nn.Dropout(f))\n",
    "\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        import itertools\n",
    "        components = list(itertools.zip_longest(self.layers,self.act_ftns, self.dropouts))\n",
    "        for layer, act_ftn, dropout in components:\n",
    "            #init components and forward through\n",
    "            if layer != None:\n",
    "                x = layer(x)\n",
    "            if act_ftn != None:\n",
    "                x = act_ftn(x)\n",
    "            if dropout != None:\n",
    "                x = dropout(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        return x\n",
    "\n",
    "#------------------ Loss & Optimizer --------------------\n",
    "def cross_entropy_loss(y_true, y_hat):\n",
    "    return torch.mean(torch.sum(- y_true * torch.log(y_hat), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Metrics ---------------------\n",
    "def cross_entropy(y_true,y_hat,  eps=1e-15):\n",
    "    return -(y_true * np.log(y_hat)).sum(axis=1).mean()\n",
    "\n",
    "def accuracy(pred, targets):\n",
    "    return sum(np.argmax(pred,axis=1) == np.argmax(targets,axis=1))/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "synthetic_labels = eval(open('synthetic_fig_5_labels.json','r').read())\n",
    "synthetic_priors = eval(open('irony/synthetic_prior_states.json').read())\n",
    "utters = [1,2,3,4,5]\n",
    "states = [1,2,3,4,5]\n",
    "\n",
    "def build_one_hot_utter(u):\n",
    "    one_hot_utter = np.zeros(len(utters))\n",
    "    one_hot_utter[utters.index(u)] = 1\n",
    "    return one_hot_utter\n",
    "\n",
    "def build_context(c,priors):\n",
    "    priors_dict = priors[c]\n",
    "    state_priors = [priors_dict[s] for s in states]\n",
    "    return state_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_out(data, priors):\n",
    "    x = []\n",
    "    y = []\n",
    "    for c, vals in data.items():\n",
    "        context_prior = build_context(c,priors)\n",
    "        for utter, state in vals.items():\n",
    "            one_hot_u = build_one_hot_utter(utter)\n",
    "            x.append(np.concatenate((context_prior,one_hot_u),axis=None))\n",
    "            y.append(state)            \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_data, num_epochs):\n",
    "    prev_loss = -100000000\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        x = Variable(torch.Tensor(training_data[0]))\n",
    "        y = Variable(torch.Tensor(training_data[1]))\n",
    "\n",
    "        y_hat = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y,y_hat)\n",
    "        print(epoch, loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if epoch > 350:\n",
    "            dev_y_hat = predict(model, dev_x)\n",
    "            dev_loss = cross_entropy(dev_y,dev_y_hat)\n",
    "            if prev_loss < dev_loss:\n",
    "                break\n",
    "            else:\n",
    "                prev_loss = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_sheet(model, train_metrics = (\"N/A\",\"N/A\",\"N/A\"), dev_metrics= (\"N/A\",\"N/A\",\"N/A\"), test_metrics= (\"N/A\",\"N/A\",\"N/A\")):\n",
    "    import gspread\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets','https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    wks = gc.open('RSA-NN').sheet1\n",
    "\n",
    "    row = str(len(wks.get_all_records()) +2)\n",
    "    \n",
    "    torch.save(model.state_dict(),SAVE_PATH+row+'.pt')\n",
    "\n",
    "    wks.append_row([\n",
    "                train_metrics[0], train_metrics[1], train_metrics[2],\n",
    "                dev_metrics[0], dev_metrics[1], dev_metrics[2],\n",
    "                test_metrics[0], test_metrics[1], test_metrics[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    x = Variable(torch.Tensor(data))\n",
    "    \n",
    "    y_hat = model(x).data.numpy()\n",
    "    row_sums = y_hat.sum(axis=1)\n",
    "    y_hat = y_hat / row_sums[:, np.newaxis]\n",
    "    return y_hat\n",
    "\n",
    "def evaluate(y,y_hat):\n",
    "    ce = cross_entropy(y,y_hat)\n",
    "    acc = accuracy(y_hat,y)\n",
    "    mse = metrics.mean_squared_error(y_hat,y)\n",
    "    print(\"ce: \", ce)\n",
    "    print(\"acc: \", acc)\n",
    "    print(\"mse: \", mse)\n",
    "    #append_to_sheet(model,(acc,mse,ce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = inp_out(synthetic_labels, synthetic_priors)\n",
    "\n",
    "split_1 = int(.8 * len(x))\n",
    "split_2 = int(.9 * len(x))\n",
    "\n",
    "train_x, train_y = x[:split_1], y[:split_1]\n",
    "dev_x, dev_y = x[split_1:split_2], y[split_1:split_2]\n",
    "test_x, test_y = x[split_2:], y[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.0365256577383264\n",
      "acc:  0.9525\n",
      "mse:  0.0007171337404917757\n",
      "24 1.5860933065414429\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.1123113583747073\n",
      "acc:  0.8625\n",
      "mse:  0.006070090217469137\n",
      "53 1.0600376129150395\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.0234051260229162\n",
      "acc:  0.9875\n",
      "mse:  0.00014475210691958343\n"
     ]
    }
   ],
   "source": [
    "final_model = None\n",
    "final_model_ce = 1000000\n",
    "\n",
    "loss_fn = cross_entropy_loss\n",
    "for learning_rate in [1e-3,1e-4,1e-2]:\n",
    "    for nonlin in [[nn.Tanh(),nn.ReLU()]]:\n",
    "        for num_units in [[70,80,90]]:\n",
    "            model = Neural_Net(train_x.shape[1],train_y.shape[1],layer_lens=num_units,\n",
    "                              nonlins=nonlin).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=.0001)\n",
    "            train(model,optimizer, loss_fn, (train_x,train_y), num_epochs=NUM_EPOCHS)\n",
    "            y_hat = predict(model,train_x)\n",
    "            evaluate(train_y,y_hat)\n",
    "            \n",
    "            #find optimal model\n",
    "            ce = cross_entropy(train_y,y_hat)\n",
    "            if ce < final_model_ce:\n",
    "                final_model = model\n",
    "                final_model_ce = ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"irony_models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.0976889162806824\n",
      "acc:  0.94\n",
      "mse:  0.0015724501474155001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "y_hat = predict(model,test_x)\n",
    "evaluate(test_y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.0041271231281907\n",
      "acc:  0.9555555555555556\n",
      "mse:  0.00416327250902703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "fig_5_labels = eval(open('fig_5_labels.json','r').read())\n",
    "priors = eval(open('irony/prior_states.json').read())\n",
    "\n",
    "x_real,y_real = inp_out(fig_5_labels, priors)\n",
    "y_hat = predict(model,x_real)\n",
    "evaluate(y_real,y_hat)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
