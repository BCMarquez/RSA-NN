{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=100)\n",
    "import json\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and load GoogleNews key2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "key2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "weights = key2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = key2vec.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing replacement words so that we have minimal overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disgraceful', 0.7855889201164246),\n",
       " ('horrendous', 0.7606023550033569),\n",
       " ('deplorable', 0.754478394985199),\n",
       " ('atrocious', 0.749769926071167),\n",
       " ('shameful', 0.72076416015625),\n",
       " ('appaling', 0.7162660360336304),\n",
       " ('disgusting', 0.7104288935661316),\n",
       " ('abominable', 0.7005642652511597),\n",
       " ('totally_unacceptable', 0.6859470009803772),\n",
       " ('dreadful', 0.6839314699172974)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2vec.most_similar(\"appalling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lousy', 0.8046033382415771),\n",
       " ('crappy', 0.7873436212539673),\n",
       " ('shitty', 0.6884918212890625),\n",
       " ('cruddy', 0.67264723777771),\n",
       " ('mediocre', 0.6025679707527161),\n",
       " ('horrid', 0.5934531688690186),\n",
       " ('awful', 0.5819990038871765),\n",
       " ('bad', 0.5677820444107056),\n",
       " ('miserable', 0.566582202911377),\n",
       " ('god_awful', 0.5625608563423157)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2vec.most_similar(\"crummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alright', 0.8877537250518799),\n",
       " ('ok', 0.8567795753479004),\n",
       " ('OK', 0.7831767797470093),\n",
       " ('yeah', 0.6638473272323608),\n",
       " ('allright', 0.6537948846817017),\n",
       " ('hey', 0.603424608707428),\n",
       " ('say_Hey_feller', 0.5872976779937744),\n",
       " ('anyway', 0.5856851935386658),\n",
       " ('anyways', 0.5801851153373718),\n",
       " ('maybe', 0.5797765851020813)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2vec.most_similar(\"okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Decent', 0.7122883796691895),\n",
       " ('good', 0.6837348341941833),\n",
       " ('respectable', 0.6686230301856995),\n",
       " ('decently', 0.6438794136047363),\n",
       " ('mediocre', 0.6006860733032227),\n",
       " ('terrific', 0.5998380184173584),\n",
       " ('nice', 0.5993332266807556),\n",
       " ('solid', 0.5919034481048584),\n",
       " ('middling', 0.5665558576583862),\n",
       " ('excellent', 0.5584126710891724)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2vec.most_similar(\"decent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.8353004455566406),\n",
       " ('lovely', 0.8106936812400818),\n",
       " ('stunningly_beautiful', 0.7329413890838623),\n",
       " ('breathtakingly_beautiful', 0.7231341600418091),\n",
       " ('wonderful', 0.6854087114334106),\n",
       " ('fabulous', 0.6700063943862915),\n",
       " ('loveliest', 0.6612576246261597),\n",
       " ('prettiest', 0.6595001816749573),\n",
       " ('beatiful', 0.6593325138092041),\n",
       " ('magnificent', 0.6591403484344482)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2vec.most_similar(\"beautiful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = 'models/model_'\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# all the one-hot embedding etc. has to be handled here, rather in data generation,\n",
    "# because we have to know which word indice go to which real words in order to do the embedding.\n",
    "\n",
    "#--------------- Model --------------\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_lens = [25,40,50], nonlins = [nn.Tanh(), nn.ReLU()], drop_freqs= []):\n",
    "        super(Neural_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.act_ftns = nonlins\n",
    "        self.dropouts = []\n",
    "\n",
    "        curr_layer_len = input_size\n",
    "        for l in layer_lens:\n",
    "            self.layers.append(nn.Linear(curr_layer_len, l, bias = True))\n",
    "            curr_layer_len = l\n",
    "        self.layers.append( nn.Linear(curr_layer_len,self.output_size))\n",
    "\n",
    "        for f in drop_freqs:\n",
    "             self.dropouts.append(nn.Dropout(f))\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        import itertools\n",
    "        components = list(itertools.zip_longest(self.layers,self.act_ftns, self.dropouts))\n",
    "        \n",
    "        for layer, act_ftn, dropout in components:\n",
    "            #init components and forward through\n",
    "            if layer != None:\n",
    "                x = layer(x)\n",
    "            if act_ftn != None:\n",
    "                x = act_ftn(x)\n",
    "            if dropout != None:\n",
    "                x = dropout(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        return x\n",
    "\n",
    "#------------------ Loss & Optimizer --------------------\n",
    "def cross_entropy_loss(y_true, y_hat):\n",
    "    return torch.mean(torch.sum(- y_true * torch.log(y_hat), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_data, num_epochs):\n",
    "    prev_loss = -100000000\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        x = Variable(torch.Tensor(training_data[0]))\n",
    "        y = Variable(torch.Tensor(training_data[1]))\n",
    "\n",
    "        y_hat = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y,y_hat)\n",
    "        print(epoch, loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        \n",
    "        if epoch > 350:\n",
    "            dev_y_hat = predict(model, dev_x)\n",
    "            dev_loss = cross_entropy(dev_y,dev_y_hat)\n",
    "            if prev_loss < dev_loss:\n",
    "                break\n",
    "            else:\n",
    "                prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Metrics ---------------------\n",
    "def cross_entropy(y_true,y_hat,  eps=1e-15):\n",
    "    return -(y_true * np.log(y_hat)).sum(axis=1).mean()\n",
    "\n",
    "# accuracy returns proportion of max(preds) in max(targets)\n",
    "def accuracy(pred, targets):\n",
    "    top_targs = (np.max(targets,axis=1)[:,None] == targets) #set indexes with top values as true\n",
    "    top_preds = (np.max(pred,axis=1)[:,None] == pred) #set indexes with top values as true\n",
    "    correct_preds = np.sum(top_targs&top_preds,axis=1) #counts intersections of True\n",
    "    total_top_preds = np.sum(top_preds,axis=1)\n",
    "    accuracy_ratio = correct_preds/total_top_preds\n",
    "    return accuracy_ratio.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data into variables\n",
    "synthetic_labels = eval(open('synthetic_fig_5_labels.json','r').read())\n",
    "synthetic_priors = eval(open('irony/synthetic_prior_states.json').read())\n",
    "\n",
    "utters = {1:\"terrible\",2:\"bad\",3:\"neutral\",4:\"good\",5:\"amazing\"}\n",
    "states = [1,2,3,4,5]\n",
    "\n",
    "#function remodified to provide an embedding for utterance passed in. \n",
    "#Name will be updated accordingly\n",
    "def build_one_hot_utter(u):    \n",
    "    return weights[key2vec.vocab[u].index]\n",
    "\n",
    "def build_context(c,priors):\n",
    "    priors_dict = priors[c]\n",
    "    state_priors = [priors_dict[s] for s in states]\n",
    "    return state_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn json data file into input/output matrices we can work with\n",
    "def inp_out(data, priors):\n",
    "    x = []\n",
    "    y = []\n",
    "    for c, vals in data.items():\n",
    "        context_prior = build_context(c,priors)\n",
    "        for utter, state in vals.items():\n",
    "            one_hot_u = build_one_hot_utter(utters[utter])\n",
    "            x.append(np.concatenate((context_prior,one_hot_u),axis=None))\n",
    "            y.append(state)            \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on data using a trained model\n",
    "def predict(model, data):\n",
    "    x = Variable(torch.Tensor(data))\n",
    "    \n",
    "    y_hat = model(x).data.numpy()\n",
    "    row_sums = y_hat.sum(axis=1)\n",
    "    y_hat = y_hat / row_sums[:, np.newaxis]\n",
    "    return y_hat\n",
    "\n",
    "#prints out evaluation metrics\n",
    "def evaluate(y,y_hat):\n",
    "    ce = cross_entropy(y,y_hat)\n",
    "    acc = accuracy(y_hat,y)\n",
    "    mse = metrics.mean_squared_error(y_hat,y)\n",
    "    print(\"ce: \", ce)\n",
    "    print(\"acc: \", acc)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn json data into matrices\n",
    "x,y = inp_out(synthetic_labels, synthetic_priors)\n",
    "\n",
    "#split for training\n",
    "split_1 = int(.8 * len(x))\n",
    "split_2 = int(.9 * len(x))\n",
    "\n",
    "train_x, train_y = x[:split_1], y[:split_1]\n",
    "dev_x, dev_y = x[split_1:split_2], y[split_1:split_2]\n",
    "test_x, test_y = x[split_2:], y[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  1.0654976555102087\n",
      "acc:  0.965\n",
      "mse:  0.00019486643031936158\n"
     ]
    }
   ],
   "source": [
    "#this grid search will automatically choose the model with the lowest cross entropy\n",
    "\n",
    "final_model = None\n",
    "final_model_ce = 1000000\n",
    "\n",
    "loss_fn = cross_entropy_loss\n",
    "for learning_rate in [1e-2]:\n",
    "    for nonlin in [[nn.Tanh(),nn.ReLU()]]:\n",
    "        for num_units in [[70,80,90]]:\n",
    "            model = Neural_Net(train_x.shape[1],train_y.shape[1],layer_lens=num_units,\n",
    "                              nonlins=nonlin).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=.0001)\n",
    "            train(model,optimizer, loss_fn, (train_x,train_y), num_epochs=NUM_EPOCHS)\n",
    "            y_hat = predict(model,train_x)\n",
    "            evaluate(train_y,y_hat)\n",
    "            \n",
    "            #find optimal model\n",
    "            ce = cross_entropy(train_y,y_hat)\n",
    "            if ce < final_model_ce:\n",
    "                final_model = model\n",
    "                final_model_ce = ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"irony_models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  0.8945989011238548\n",
      "acc:  0.98\n",
      "mse:  0.0012197232282003482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "y_hat = predict(model,test_x)\n",
    "evaluate(test_y,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against entropy of actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8689911294155673"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(test_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on data gathered from Mechanical Turk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  0.9481536461113158\n",
      "acc:  0.8888888888888888\n",
      "mse:  0.0007853106827594084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "fig_5_labels = eval(open('fig_5_labels.json','r').read())\n",
    "priors = eval(open('irony/prior_states.json').read())\n",
    "\n",
    "x_real,y_real = inp_out(fig_5_labels, priors)\n",
    "y_hat = predict(model,x_real)\n",
    "evaluate(y_real,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against entropy of Mechanical Turk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9346084394220676"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y_real,y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine utterances to test on synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "utters = {1:\"appalling\",2:\"crummy\",3:\"okay\",4:\"decent\",5:\"beautiful\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  0.8945989011238548\n",
      "acc:  0.98\n",
      "mse:  0.0012197232282003482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "x_modif,y_modif = inp_out(synthetic_labels, synthetic_priors) \n",
    "split_1 = int(.8 * len(x))\n",
    "split_2 = int(.9 * len(x))\n",
    "\n",
    "modif_test_x, modif_test_y = x_modif[split_2:], y_modif[split_2:]\n",
    "modif_y_hat = predict(model, modif_test_x)\n",
    "evaluate(modif_test_y,modif_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on held-out data with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  0.9481536461113158\n",
      "acc:  0.8888888888888888\n",
      "mse:  0.0007853106827594084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "x_real,y_real = inp_out(fig_5_labels, priors)\n",
    "y_hat = predict(model,x_real)\n",
    "evaluate(y_real,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy for a model that has no idea of what synonyms mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_pred = np.full(shape=y_modif.shape, fill_value=.20)\n",
    "accuracy(unknown_words_pred,y_modif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999993"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_pred = np.full(shape=y_real.shape, fill_value=.20)\n",
    "accuracy(unknown_words_pred,y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate CE for a model that has no idea of what synonyms mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6094379124340998"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_pred = np.full(shape=y_modif.shape, fill_value=.20)\n",
    "cross_entropy(y_modif,unknown_words_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6094379124341007"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_pred = np.full(shape=y_real.shape, fill_value=.20)\n",
    "cross_entropy(y_real,unknown_words_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
