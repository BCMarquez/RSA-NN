{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import json\n",
    "import torchtext.vocab\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# all the one-hot embedding etc. has to be handled here, rather in data generation,\n",
    "# because we have to know which word indice go to which real words in order to do the embedding.\n",
    "\n",
    "#--------------- Model --------------\n",
    "class Neural_Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_lens = [30,40,50], nonlins = [nn.Tanh(), nn.ReLU()], drop_freqs= [.5,.6]):\n",
    "        super(Neural_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.act_ftns = nonlins\n",
    "        self.dropouts = []\n",
    "\n",
    "        curr_layer_len = input_size\n",
    "        for l in layer_lens:\n",
    "            self.layers.append(nn.Linear(curr_layer_len, l, bias = True))\n",
    "            curr_layer_len = l\n",
    "        self.layers.append( nn.Linear(curr_layer_len,self.output_size))\n",
    "\n",
    "        for f in drop_freqs:\n",
    "             self.dropouts.append(nn.Dropout(f))\n",
    "\n",
    "               \n",
    "\n",
    "    def forward(self, x):\n",
    "        import itertools\n",
    "        components = list(itertools.zip_longest(self.layers,self.act_ftns, self.dropouts))\n",
    "        for layer, act_ftn, dropout in components:\n",
    "            #init components and forward through\n",
    "            if layer != None:\n",
    "                x = layer(x)\n",
    "            if act_ftn != None:\n",
    "                x = act_ftn(x)\n",
    "            if dropout != None:\n",
    "                x = dropout(x)\n",
    "        x = nn.LogSoftmax()(x)\n",
    "        return x\n",
    "\n",
    "#------------------ Loss & Optimizer --------------------\n",
    "def cross_entropy_loss(logp_hats, ps):\n",
    "    return torch.mean(torch.sum(- ps * logp_hats, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------- Metrics ---------------------\n",
    "def cross_entropy(log_pred, targets):\n",
    "    log_pred = Variable(torch.Tensor(log_pred))\n",
    "    targets = Variable(torch.Tensor(targets))\n",
    "\n",
    "    result = sum(torch.sum(-targets * log_pred, 1).data.numpy()) #I should as Professor Singh for help here as torch.sum is returning an array over the 6 instances. Does summing those before returning them make sense? And does it make sense to take the average once all those instances have been collected (as shown in line 86). Right now it doesn't make sense because cross entropy is really high compared to mse\n",
    "    return result\n",
    "\n",
    "def accuracy(pred, targets):\n",
    "    return sum(np.argmax(pred,axis=1) == np.argmax(targets,axis=1))/len(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = eval(open('fig_5_labels.json','r').read())\n",
    "contexts = list(data.keys())\n",
    "contexts.sort()\n",
    "utters = [1,2,3,4,5]\n",
    "states = [1,2,3,4,5]\n",
    "\n",
    "def build_one_hot_utter(u):\n",
    "    one_hot_utter = np.zeros(len(utters))\n",
    "    one_hot_utter[utters.index(u)] = 1\n",
    "    return one_hot_utter\n",
    "def build_one_hot_state(s):\n",
    "    one_hot_state = np.zeros(len(states))\n",
    "    one_hot_state[states.index(s)] = 1\n",
    "    return one_hot_state\n",
    "def build_one_hot_context(c):\n",
    "    one_hot_context = np.zeros(len(contexts))\n",
    "    one_hot_context[contexts.index(c)] = 1\n",
    "    return one_hot_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for context, vals in data.items():\n",
    "    one_hot_c = build_one_hot_context(context)\n",
    "    for utter, state in vals.items():\n",
    "        one_hot_u = build_one_hot_utter(utter)\n",
    "        x.append(np.concatenate((one_hot_c,one_hot_u),axis=None))\n",
    "        y.append(state)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 14)\n",
      "(45, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_data, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        x = Variable(torch.Tensor(training_data[0]))\n",
    "        y = Variable(torch.Tensor(training_data[1]))\n",
    "\n",
    "        log_y_pred = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(log_y_pred, y)\n",
    "        print(epoch, loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    x = Variable(torch.Tensor(data[0]))\n",
    "    y = data[1]\n",
    "    \n",
    "    y_hat = model(x).data.numpy()\n",
    "    \n",
    "    ce = cross_entropy(y_hat,y)\n",
    "    acc = accuracy(y_hat,y)\n",
    "    print(\"ce: \", ce)\n",
    "    print(\"acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6459275484085083\r",
      "1 1.6460647583007812\r",
      "2 1.6602647304534912\r",
      "3 1.6501364707946777\r",
      "4 1.6289926767349243\r",
      "5 1.631019949913025\r",
      "6 1.635552167892456\r",
      "7 1.6298843622207642\r",
      "8 1.6211719512939453\r",
      "9 1.6262789964675903\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brndon/anaconda3/envs/torch/lib/python3.5/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce:  44.556146785616875\n",
      "acc:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    loss_fn = cross_entropy_loss\n",
    "    for learning_rate in [1e-3,1e-4,1e-2]:\n",
    "        for nonlin in [[nn.Tanh(),nn.ReLU()]]:\n",
    "            for num_units in [[70,30],[70,80,90]]:\n",
    "                model = Neural_Net(x.shape[1],y.shape[1],layer_lens=num_units,\n",
    "                                  nonlins=nonlin).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=.0001)\n",
    "                train(model,optimizer, loss_fn, (x,y), num_epochs=NUM_EPOCHS)\n",
    "    evaluate(model,(x,y))\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
